{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763fdc85-7d0a-48a3-a5bf-1c1f43e87c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 01:08:43 - Log file created at: results/word2vec.log\n",
      "2025-02-16 01:08:43 - Starting Word2Vec training process\n",
      "2025-02-16 01:08:43 - Loading training data...\n",
      "2025-02-16 01:08:43 - Loading from: train/positive\n",
      "2025-02-16 01:08:44 - Loading from: train/negative\n",
      "2025-02-16 01:08:45 - Loaded 2904 texts from train set\n",
      "2025-02-16 01:08:45 - Building vocabulary...\n",
      "2025-02-16 01:08:45 - Vocabulary size: 6262\n",
      "2025-02-16 01:08:45 - Generating training pairs...\n",
      "2025-02-16 01:09:02 - Generated 568932 training pairs\n",
      "2025-02-16 01:09:02 - \n",
      "Training Logistic Regression model...\n",
      "2025-02-16 01:09:03 - Starting training with 17780 total batches\n",
      "Epoch 0: 100%|██████████| 17780/17780 [00:29<00:00, 598.31it/s] \n",
      "2025-02-16 01:09:33 - Epoch 0 completed:\n",
      "2025-02-16 01:09:33 -   Total Loss: 10406.3938\n",
      "2025-02-16 01:09:33 -   Average batch loss: 0.5853\n",
      "2025-02-16 01:09:33 -   Number of batches: 17780\n",
      "2025-02-16 01:09:33 -   Learning rate: 0.1\n",
      "Epoch 1: 100%|██████████| 17780/17780 [00:10<00:00, 1665.22it/s]\n",
      "2025-02-16 01:09:44 - Epoch 1 completed:\n",
      "2025-02-16 01:09:44 -   Total Loss: 9130.5904\n",
      "2025-02-16 01:09:44 -   Average batch loss: 0.5135\n",
      "2025-02-16 01:09:44 -   Number of batches: 17780\n",
      "2025-02-16 01:09:44 -   Learning rate: 0.1\n",
      "Epoch 2: 100%|██████████| 17780/17780 [00:18<00:00, 982.39it/s] \n",
      "2025-02-16 01:10:02 - Epoch 2 completed:\n",
      "2025-02-16 01:10:02 -   Total Loss: 8421.0180\n",
      "2025-02-16 01:10:02 -   Average batch loss: 0.4736\n",
      "2025-02-16 01:10:02 -   Number of batches: 17780\n",
      "2025-02-16 01:10:02 -   Learning rate: 0.1\n",
      "Epoch 3: 100%|██████████| 17780/17780 [01:25<00:00, 208.49it/s] \n",
      "2025-02-16 01:11:27 - Epoch 3 completed:\n",
      "2025-02-16 01:11:27 -   Total Loss: 7956.3461\n",
      "2025-02-16 01:11:27 -   Average batch loss: 0.4475\n",
      "2025-02-16 01:11:27 -   Number of batches: 17780\n",
      "2025-02-16 01:11:27 -   Learning rate: 0.1\n",
      "Epoch 4: 100%|██████████| 17780/17780 [00:28<00:00, 625.20it/s] \n",
      "2025-02-16 01:11:56 - Epoch 4 completed:\n",
      "2025-02-16 01:11:56 -   Total Loss: 7627.0126\n",
      "2025-02-16 01:11:56 -   Average batch loss: 0.4290\n",
      "2025-02-16 01:11:56 -   Number of batches: 17780\n",
      "2025-02-16 01:11:56 -   Learning rate: 0.1\n",
      "Epoch 5: 100%|██████████| 17780/17780 [00:30<00:00, 578.75it/s] \n",
      "2025-02-16 01:12:26 - Epoch 5 completed:\n",
      "2025-02-16 01:12:26 -   Total Loss: 7374.9061\n",
      "2025-02-16 01:12:26 -   Average batch loss: 0.4148\n",
      "2025-02-16 01:12:26 -   Number of batches: 17780\n",
      "2025-02-16 01:12:26 -   Learning rate: 0.1\n",
      "Epoch 6: 100%|██████████| 17780/17780 [01:00<00:00, 293.45it/s] \n",
      "2025-02-16 01:13:27 - Epoch 6 completed:\n",
      "2025-02-16 01:13:27 -   Total Loss: 7181.1352\n",
      "2025-02-16 01:13:27 -   Average batch loss: 0.4039\n",
      "2025-02-16 01:13:27 -   Number of batches: 17780\n",
      "2025-02-16 01:13:27 -   Learning rate: 0.1\n",
      "Epoch 7: 100%|██████████| 17780/17780 [00:18<00:00, 972.70it/s] \n",
      "2025-02-16 01:13:45 - Epoch 7 completed:\n",
      "2025-02-16 01:13:45 -   Total Loss: 7023.2234\n",
      "2025-02-16 01:13:45 -   Average batch loss: 0.3950\n",
      "2025-02-16 01:13:45 -   Number of batches: 17780\n",
      "2025-02-16 01:13:45 -   Learning rate: 0.1\n",
      "Epoch 8: 100%|██████████| 17780/17780 [01:51<00:00, 159.13it/s] \n",
      "2025-02-16 01:15:37 - Epoch 8 completed:\n",
      "2025-02-16 01:15:37 -   Total Loss: 6899.8070\n",
      "2025-02-16 01:15:37 -   Average batch loss: 0.3881\n",
      "2025-02-16 01:15:37 -   Number of batches: 17780\n",
      "2025-02-16 01:15:37 -   Learning rate: 0.1\n",
      "Epoch 9: 100%|██████████| 17780/17780 [01:13<00:00, 240.57it/s] \n",
      "2025-02-16 01:16:51 - Epoch 9 completed:\n",
      "2025-02-16 01:16:51 -   Total Loss: 6794.3584\n",
      "2025-02-16 01:16:51 -   Average batch loss: 0.3821\n",
      "2025-02-16 01:16:51 -   Number of batches: 17780\n",
      "2025-02-16 01:16:51 -   Learning rate: 0.1\n",
      "Epoch 10: 100%|██████████| 17780/17780 [01:16<00:00, 231.52it/s] \n",
      "2025-02-16 01:18:08 - Epoch 10 completed:\n",
      "2025-02-16 01:18:08 -   Total Loss: 6707.3062\n",
      "2025-02-16 01:18:08 -   Average batch loss: 0.3772\n",
      "2025-02-16 01:18:08 -   Number of batches: 17780\n",
      "2025-02-16 01:18:08 -   Learning rate: 0.1\n",
      "Epoch 11: 100%|██████████| 17780/17780 [00:27<00:00, 656.19it/s] \n",
      "2025-02-16 01:18:35 - Epoch 11 completed:\n",
      "2025-02-16 01:18:35 -   Total Loss: 6629.3347\n",
      "2025-02-16 01:18:35 -   Average batch loss: 0.3729\n",
      "2025-02-16 01:18:35 -   Number of batches: 17780\n",
      "2025-02-16 01:18:35 -   Learning rate: 0.1\n",
      "Epoch 12: 100%|██████████| 17780/17780 [00:11<00:00, 1506.11it/s]\n",
      "2025-02-16 01:18:47 - Epoch 12 completed:\n",
      "2025-02-16 01:18:47 -   Total Loss: 6568.1045\n",
      "2025-02-16 01:18:47 -   Average batch loss: 0.3694\n",
      "2025-02-16 01:18:47 -   Number of batches: 17780\n",
      "2025-02-16 01:18:47 -   Learning rate: 0.1\n",
      "Epoch 13: 100%|██████████| 17780/17780 [00:16<00:00, 1079.39it/s]\n",
      "2025-02-16 01:19:03 - Epoch 13 completed:\n",
      "2025-02-16 01:19:03 -   Total Loss: 6509.3532\n",
      "2025-02-16 01:19:03 -   Average batch loss: 0.3661\n",
      "2025-02-16 01:19:03 -   Number of batches: 17780\n",
      "2025-02-16 01:19:03 -   Learning rate: 0.1\n",
      "Epoch 14: 100%|██████████| 17780/17780 [00:11<00:00, 1489.22it/s]\n",
      "2025-02-16 01:19:15 - Epoch 14 completed:\n",
      "2025-02-16 01:19:15 -   Total Loss: 6463.4616\n",
      "2025-02-16 01:19:15 -   Average batch loss: 0.3635\n",
      "2025-02-16 01:19:15 -   Number of batches: 17780\n",
      "2025-02-16 01:19:15 -   Learning rate: 0.1\n",
      "Epoch 15: 100%|██████████| 17780/17780 [01:25<00:00, 207.98it/s] \n",
      "2025-02-16 01:20:41 - Epoch 15 completed:\n",
      "2025-02-16 01:20:41 -   Total Loss: 6421.1740\n",
      "2025-02-16 01:20:41 -   Average batch loss: 0.3611\n",
      "2025-02-16 01:20:41 -   Number of batches: 17780\n",
      "2025-02-16 01:20:41 -   Learning rate: 0.1\n",
      "Epoch 16: 100%|██████████| 17780/17780 [01:08<00:00, 258.80it/s] \n",
      "2025-02-16 01:21:50 - Epoch 16 completed:\n",
      "2025-02-16 01:21:50 -   Total Loss: 6381.6439\n",
      "2025-02-16 01:21:50 -   Average batch loss: 0.3589\n",
      "2025-02-16 01:21:50 -   Number of batches: 17780\n",
      "2025-02-16 01:21:50 -   Learning rate: 0.1\n",
      "Epoch 17: 100%|██████████| 17780/17780 [00:10<00:00, 1634.24it/s]\n",
      "2025-02-16 01:22:00 - Epoch 17 completed:\n",
      "2025-02-16 01:22:00 -   Total Loss: 6351.4054\n",
      "2025-02-16 01:22:00 -   Average batch loss: 0.3572\n",
      "2025-02-16 01:22:00 -   Number of batches: 17780\n",
      "2025-02-16 01:22:00 -   Learning rate: 0.1\n",
      "Epoch 18: 100%|██████████| 17780/17780 [00:44<00:00, 400.68it/s] \n",
      "2025-02-16 01:22:45 - Epoch 18 completed:\n",
      "2025-02-16 01:22:45 -   Total Loss: 6319.8669\n",
      "2025-02-16 01:22:45 -   Average batch loss: 0.3554\n",
      "2025-02-16 01:22:45 -   Number of batches: 17780\n",
      "2025-02-16 01:22:45 -   Learning rate: 0.1\n",
      "Epoch 19: 100%|██████████| 17780/17780 [00:23<00:00, 762.97it/s] \n",
      "2025-02-16 01:23:08 - Epoch 19 completed:\n",
      "2025-02-16 01:23:08 -   Total Loss: 6291.3770\n",
      "2025-02-16 01:23:08 -   Average batch loss: 0.3538\n",
      "2025-02-16 01:23:08 -   Number of batches: 17780\n",
      "2025-02-16 01:23:08 -   Learning rate: 0.1\n",
      "2025-02-16 01:23:08 - \n",
      "Training FFNN model...\n",
      "2025-02-16 01:23:08 - Starting training with 17780 total batches\n",
      "Epoch 0: 100%|██████████| 17780/17780 [01:13<00:00, 243.52it/s] \n",
      "2025-02-16 01:24:21 - Epoch 0 completed:\n",
      "2025-02-16 01:24:21 -   Total Loss: 7375.5382\n",
      "2025-02-16 01:24:21 -   Average batch loss: 0.4148\n",
      "2025-02-16 01:24:21 -   Number of batches: 17780\n",
      "2025-02-16 01:24:21 -   Learning rate: 0.1\n",
      "Epoch 1: 100%|██████████| 17780/17780 [01:24<00:00, 211.40it/s] \n",
      "2025-02-16 01:25:45 - Epoch 1 completed:\n",
      "2025-02-16 01:25:45 -   Total Loss: 6422.0516\n",
      "2025-02-16 01:25:45 -   Average batch loss: 0.3612\n",
      "2025-02-16 01:25:45 -   Number of batches: 17780\n",
      "2025-02-16 01:25:45 -   Learning rate: 0.1\n",
      "Epoch 2: 100%|██████████| 17780/17780 [00:26<00:00, 675.76it/s] \n",
      "2025-02-16 01:26:12 - Epoch 2 completed:\n",
      "2025-02-16 01:26:12 -   Total Loss: 6248.1849\n",
      "2025-02-16 01:26:12 -   Average batch loss: 0.3514\n",
      "2025-02-16 01:26:12 -   Number of batches: 17780\n",
      "2025-02-16 01:26:12 -   Learning rate: 0.1\n",
      "Epoch 3: 100%|██████████| 17780/17780 [01:09<00:00, 255.93it/s] \n",
      "2025-02-16 01:27:21 - Epoch 3 completed:\n",
      "2025-02-16 01:27:21 -   Total Loss: 6160.3357\n",
      "2025-02-16 01:27:21 -   Average batch loss: 0.3465\n",
      "2025-02-16 01:27:21 -   Number of batches: 17780\n",
      "2025-02-16 01:27:21 -   Learning rate: 0.1\n",
      "Epoch 4: 100%|██████████| 17780/17780 [01:12<00:00, 246.30it/s] \n",
      "2025-02-16 01:28:33 - Epoch 4 completed:\n",
      "2025-02-16 01:28:33 -   Total Loss: 6108.1722\n",
      "2025-02-16 01:28:33 -   Average batch loss: 0.3435\n",
      "2025-02-16 01:28:33 -   Number of batches: 17780\n",
      "2025-02-16 01:28:33 -   Learning rate: 0.1\n",
      "Epoch 5: 100%|██████████| 17780/17780 [00:36<00:00, 485.54it/s] \n",
      "2025-02-16 01:29:10 - Epoch 5 completed:\n",
      "2025-02-16 01:29:10 -   Total Loss: 6063.9389\n",
      "2025-02-16 01:29:10 -   Average batch loss: 0.3411\n",
      "2025-02-16 01:29:10 -   Number of batches: 17780\n",
      "2025-02-16 01:29:10 -   Learning rate: 0.1\n",
      "Epoch 6: 100%|██████████| 17780/17780 [00:16<00:00, 1104.74it/s]\n",
      "2025-02-16 01:29:26 - Epoch 6 completed:\n",
      "2025-02-16 01:29:26 -   Total Loss: 6031.2488\n",
      "2025-02-16 01:29:26 -   Average batch loss: 0.3392\n",
      "2025-02-16 01:29:26 -   Number of batches: 17780\n",
      "2025-02-16 01:29:26 -   Learning rate: 0.1\n",
      "Epoch 7: 100%|██████████| 17780/17780 [00:49<00:00, 358.63it/s] \n",
      "2025-02-16 01:30:16 - Epoch 7 completed:\n",
      "2025-02-16 01:30:16 -   Total Loss: 6005.7354\n",
      "2025-02-16 01:30:16 -   Average batch loss: 0.3378\n",
      "2025-02-16 01:30:16 -   Number of batches: 17780\n",
      "2025-02-16 01:30:16 -   Learning rate: 0.1\n",
      "Epoch 8: 100%|██████████| 17780/17780 [00:14<00:00, 1225.00it/s]\n",
      "2025-02-16 01:30:30 - Epoch 8 completed:\n",
      "2025-02-16 01:30:30 -   Total Loss: 5981.1240\n",
      "2025-02-16 01:30:30 -   Average batch loss: 0.3364\n",
      "2025-02-16 01:30:30 -   Number of batches: 17780\n",
      "2025-02-16 01:30:30 -   Learning rate: 0.1\n",
      "Epoch 9: 100%|██████████| 17780/17780 [00:24<00:00, 712.54it/s] \n",
      "2025-02-16 01:30:55 - Epoch 9 completed:\n",
      "2025-02-16 01:30:55 -   Total Loss: 5960.4111\n",
      "2025-02-16 01:30:55 -   Average batch loss: 0.3352\n",
      "2025-02-16 01:30:55 -   Number of batches: 17780\n",
      "2025-02-16 01:30:55 -   Learning rate: 0.1\n",
      "Epoch 10: 100%|██████████| 17780/17780 [00:14<00:00, 1191.36it/s]\n",
      "2025-02-16 01:31:10 - Epoch 10 completed:\n",
      "2025-02-16 01:31:10 -   Total Loss: 5940.7312\n",
      "2025-02-16 01:31:10 -   Average batch loss: 0.3341\n",
      "2025-02-16 01:31:10 -   Number of batches: 17780\n",
      "2025-02-16 01:31:10 -   Learning rate: 0.1\n",
      "Epoch 11: 100%|██████████| 17780/17780 [01:08<00:00, 258.80it/s] \n",
      "2025-02-16 01:32:19 - Epoch 11 completed:\n",
      "2025-02-16 01:32:19 -   Total Loss: 5924.8810\n",
      "2025-02-16 01:32:19 -   Average batch loss: 0.3332\n",
      "2025-02-16 01:32:19 -   Number of batches: 17780\n",
      "2025-02-16 01:32:19 -   Learning rate: 0.1\n",
      "Epoch 12: 100%|██████████| 17780/17780 [02:05<00:00, 141.36it/s] \n",
      "2025-02-16 01:34:25 - Epoch 12 completed:\n",
      "2025-02-16 01:34:25 -   Total Loss: 5907.2667\n",
      "2025-02-16 01:34:25 -   Average batch loss: 0.3322\n",
      "2025-02-16 01:34:25 -   Number of batches: 17780\n",
      "2025-02-16 01:34:25 -   Learning rate: 0.1\n",
      "Epoch 13: 100%|██████████| 17780/17780 [00:50<00:00, 350.31it/s] \n",
      "2025-02-16 01:35:16 - Epoch 13 completed:\n",
      "2025-02-16 01:35:16 -   Total Loss: 5890.1964\n",
      "2025-02-16 01:35:16 -   Average batch loss: 0.3313\n",
      "2025-02-16 01:35:16 -   Number of batches: 17780\n",
      "2025-02-16 01:35:16 -   Learning rate: 0.1\n",
      "Epoch 14: 100%|██████████| 17780/17780 [01:08<00:00, 260.60it/s] \n",
      "2025-02-16 01:36:24 - Epoch 14 completed:\n",
      "2025-02-16 01:36:24 -   Total Loss: 5870.7383\n",
      "2025-02-16 01:36:24 -   Average batch loss: 0.3302\n",
      "2025-02-16 01:36:24 -   Number of batches: 17780\n",
      "2025-02-16 01:36:24 -   Learning rate: 0.1\n",
      "Epoch 15: 100%|██████████| 17780/17780 [01:38<00:00, 181.20it/s] \n",
      "2025-02-16 01:38:02 - Epoch 15 completed:\n",
      "2025-02-16 01:38:02 -   Total Loss: 5861.0307\n",
      "2025-02-16 01:38:02 -   Average batch loss: 0.3296\n",
      "2025-02-16 01:38:02 -   Number of batches: 17780\n",
      "2025-02-16 01:38:02 -   Learning rate: 0.1\n",
      "Epoch 16: 100%|██████████| 17780/17780 [00:44<00:00, 397.06it/s] \n",
      "2025-02-16 01:38:47 - Epoch 16 completed:\n",
      "2025-02-16 01:38:47 -   Total Loss: 5844.2465\n",
      "2025-02-16 01:38:47 -   Average batch loss: 0.3287\n",
      "2025-02-16 01:38:47 -   Number of batches: 17780\n",
      "2025-02-16 01:38:47 -   Learning rate: 0.1\n",
      "Epoch 17: 100%|██████████| 17780/17780 [00:15<00:00, 1129.58it/s]\n",
      "2025-02-16 01:39:03 - Epoch 17 completed:\n",
      "2025-02-16 01:39:03 -   Total Loss: 5830.1188\n",
      "2025-02-16 01:39:03 -   Average batch loss: 0.3279\n",
      "2025-02-16 01:39:03 -   Number of batches: 17780\n",
      "2025-02-16 01:39:03 -   Learning rate: 0.1\n",
      "Epoch 18: 100%|██████████| 17780/17780 [00:25<00:00, 699.88it/s] \n",
      "2025-02-16 01:39:28 - Epoch 18 completed:\n",
      "2025-02-16 01:39:28 -   Total Loss: 5814.9048\n",
      "2025-02-16 01:39:28 -   Average batch loss: 0.3270\n",
      "2025-02-16 01:39:28 -   Number of batches: 17780\n",
      "2025-02-16 01:39:28 -   Learning rate: 0.1\n",
      "Epoch 19: 100%|██████████| 17780/17780 [01:41<00:00, 175.14it/s] \n",
      "2025-02-16 01:41:10 - Epoch 19 completed:\n",
      "2025-02-16 01:41:10 -   Total Loss: 5800.2994\n",
      "2025-02-16 01:41:10 -   Average batch loss: 0.3262\n",
      "2025-02-16 01:41:10 -   Number of batches: 17780\n",
      "2025-02-16 01:41:10 -   Learning rate: 0.1\n",
      "2025-02-16 01:41:10 - \n",
      "Starting model evaluation...\n",
      "2025-02-16 01:41:10 - Loading from: test/positive\n",
      "2025-02-16 01:41:10 - Loading from: test/negative\n",
      "2025-02-16 01:41:11 - Loaded 3591 texts from test set\n",
      "2025-02-16 01:41:11 - \n",
      "Evaluating LogiR model...\n",
      "2025-02-16 01:41:34 - Evaluation results saved to: results/logir_evaluation.txt\n",
      "2025-02-16 01:41:34 - \n",
      "Evaluating FFNN model...\n",
      "2025-02-16 01:42:11 - Evaluation results saved to: results/ffnn_evaluation.txt\n",
      "2025-02-16 01:42:11 - \n",
      "Comparing embeddings between models...\n",
      "2025-02-16 01:42:13 - Comprehensive evaluation report saved to: results/evaluation_report.txt\n",
      "2025-02-16 01:42:13 - \n",
      "Generating loss charts...\n",
      "2025-02-16 01:42:13 - Loss charts saved to: results/loss_charts.png\n",
      "2025-02-16 01:42:13 - \n",
      "Generating visualizations...\n",
      "2025-02-16 01:42:13 - Attempting LogiR visualization...\n",
      "2025-02-16 01:42:13 - Starting visualization process...\n",
      "2025-02-16 01:42:29 - Visualization saved to: results/embeddings_visualization_Logit.png\n",
      "2025-02-16 01:42:29 - \n",
      "Attempting FFNN visualization...\n",
      "2025-02-16 01:42:29 - Starting visualization process...\n",
      "2025-02-16 01:42:33 - Visualization saved to: results/embeddings_visualization_FFNN.png\n",
      "2025-02-16 01:42:33 - Training and evaluation process completed!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Imports and Constants\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.nn.functional import cosine_similarity as torch_cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "EMBEDDING_DIM = 50\n",
    "CONTEXT_WINDOW = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "# CELL 2: Logging Setup\n",
    "def setup_logging(output_dir):\n",
    "    \"\"\"Configure logging to both file and console\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers = []\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    log_file = os.path.join(output_dir, 'word2vec.log')\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    logger.info(f\"Log file created at: {log_file}\")\n",
    "    return logger\n",
    "\n",
    "\n",
    "\n",
    "# CELL 3: Data Processing Functions\n",
    "def load_data(subset='train'):\n",
    "    \"\"\"Load data from positive and negative folders\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    texts = []\n",
    "    for label in ['positive', 'negative']:\n",
    "        folder_path = os.path.join(subset, label)\n",
    "        logger.info(f\"Loading from: {folder_path}\")\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            raise Exception(f\"Directory not found: {folder_path}\")\n",
    "            \n",
    "        file_pattern = os.path.join(folder_path, '*.txt')\n",
    "        files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(f\"No text files found in {folder_path}\")\n",
    "            \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read().strip())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Loaded {len(texts)} texts from {subset} set\")\n",
    "    return texts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text with specific handling for cases and punctuation\"\"\"\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        word = re.sub(r'@', '', word)\n",
    "        punctuation = '!?.,;:()[]{}\"\"\\'\\'``'\n",
    "        \n",
    "        if any(p in word for p in punctuation):\n",
    "            parts = re.findall(r'\\w+|[' + punctuation + ']', word)\n",
    "            for part in parts:\n",
    "                if part in punctuation:\n",
    "                    tokens.extend([part] * len(part))\n",
    "                else:\n",
    "                    if not part.isupper():\n",
    "                        part = part.lower()\n",
    "                    tokens.append(part)\n",
    "        else:\n",
    "            if not word.isupper():\n",
    "                word = word.lower()\n",
    "            tokens.append(word)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def build_vocab(texts):\n",
    "    \"\"\"Build vocabulary from texts with word counts\"\"\"\n",
    "    word_counts = defaultdict(int)\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            word_counts[token] += 1\n",
    "    \n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items())}\n",
    "    idx2word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    return vocab, idx2word, word_counts\n",
    "\n",
    "def generate_pairs(tokens, window_size, vocab):\n",
    "    \"\"\"Generate positive and negative pairs for training\"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        target = tokens[i]\n",
    "        if target not in vocab:\n",
    "            continue\n",
    "            \n",
    "        left = max(0, i - window_size)\n",
    "        right = min(len(tokens), i + window_size + 1)\n",
    "        \n",
    "        for j in range(left, right):\n",
    "            if i != j and tokens[j] in vocab:\n",
    "                pairs.append((vocab[target], vocab[tokens[j]]))\n",
    "                labels.append(1)\n",
    "                \n",
    "                for _ in range(2):\n",
    "                    neg_idx = random.choice(list(vocab.values()))\n",
    "                    while neg_idx == vocab[target] or neg_idx == vocab[tokens[j]]:\n",
    "                        neg_idx = random.choice(list(vocab.values()))\n",
    "                    pairs.append((vocab[target], neg_idx))\n",
    "                    labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "\n",
    "# CELL 4: Dataset Class\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = torch.LongTensor(pairs)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "# CELL 5: Model Classes\n",
    "class SimpleWord2Vec_LogiR(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleWord2Vec_LogiR, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        target, context = inputs[:, 0], inputs[:, 1]\n",
    "        target_emb = self.embeddings(target)\n",
    "        context_emb = self.embeddings_context(context)\n",
    "        concat_emb = torch.cat([target_emb, context_emb], dim=1)\n",
    "        out = self.sigmoid(self.linear(concat_emb))\n",
    "        return out.squeeze()\n",
    "\n",
    "class SimpleWord2Vec_FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, node_size=64):\n",
    "        super(SimpleWord2Vec_FFNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim * 2, node_size)\n",
    "        self.linear2 = nn.Linear(node_size, node_size)\n",
    "        self.linear3 = nn.Linear(node_size, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        target, context = inputs[:, 0], inputs[:, 1]\n",
    "        target_emb = self.embeddings(target)\n",
    "        context_emb = self.embeddings_context(context)\n",
    "        \n",
    "        concat_emb = torch.cat([target_emb, context_emb], dim=1)\n",
    "        hidden1 = self.relu(self.linear1(concat_emb))\n",
    "        hidden2 = self.relu(self.linear2(hidden1))\n",
    "        out = self.sigmoid(self.linear3(hidden2))\n",
    "        return out.squeeze()\n",
    "    \n",
    "    \n",
    "# Improved training function with consistent logging and progress tracking\n",
    "def train_model(model, train_loader, lr=0.01):\n",
    "    \"\"\"Train the model with consistent progress monitoring\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    total_batches = len(train_loader)\n",
    "    logger.info(f\"Starting training with {total_batches} total batches\")\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # Use tqdm for progress tracking\n",
    "            for batch_inputs, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = loss_function(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            avg_batch_loss = epoch_loss / batch_count\n",
    "            losses.append(epoch_loss)\n",
    "            \n",
    "            # Consistent logging format\n",
    "            logger.info(f\"Epoch {epoch} completed:\")\n",
    "            logger.info(f\"  Total Loss: {epoch_loss:.4f}\")\n",
    "            logger.info(f\"  Average batch loss: {avg_batch_loss:.4f}\")\n",
    "            logger.info(f\"  Number of batches: {batch_count}\")\n",
    "            logger.info(f\"  Learning rate: {lr}\")\n",
    "            \n",
    "            # Optional: Save checkpoint after each epoch\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join('results', f'checkpoint_epoch_{epoch}.pt'))\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {str(e)}\")\n",
    "        # Save emergency checkpoint\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': losses[-1] if losses else None,\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join('results', 'emergency_checkpoint.pt'))\n",
    "        raise e\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "def visualize_embeddings(model, idx2word, num_words=30):\n",
    "    \"\"\"Visualize word embeddings using t-SNE\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    try:\n",
    "        logger.info(\"Starting visualization process...\")\n",
    "        embeddings = model.embeddings.weight.data.numpy()\n",
    "        \n",
    "        words = list(idx2word.values())[:num_words]\n",
    "        word_embeddings = embeddings[:num_words]\n",
    "        \n",
    "        perplexity = min(30, len(words) - 1)\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=250)\n",
    "        reduced_embeddings = tsne.fit_transform(word_embeddings)\n",
    "        \n",
    "        plt.clf()\n",
    "        for i, word in enumerate(words):\n",
    "            x, y = reduced_embeddings[i]\n",
    "            plt.scatter(x, y, c='blue', alpha=0.5)\n",
    "            plt.annotate(word, (x, y), fontsize=8, alpha=0.75)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in visualization: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def plot_losses(logir_losses, ffnn_losses, output_dir):\n",
    "    \"\"\"Plot training losses for both models\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(len(logir_losses))\n",
    "    \n",
    "    # Plot both losses on the same graph\n",
    "    plt.plot(epochs, logir_losses, 'b-', label='Logistic Regression')\n",
    "    plt.plot(epochs, ffnn_losses, 'r-', label='FFNN')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = os.path.join(output_dir, 'loss_charts.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"Loss charts saved to: {save_path}\")\n",
    "    return save_path\n",
    "    \n",
    "\n",
    "# CELL 7 (continued): Evaluation Functions\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_pairs), batch_size):\n",
    "            batch_pairs = test_pairs[i:i + batch_size]\n",
    "            target_emb = embeddings[batch_pairs[:, 0]]\n",
    "            context_emb = embeddings[batch_pairs[:, 1]]\n",
    "            sim = torch_cosine_similarity(target_emb, context_emb, dim=1)\n",
    "            similarities.extend(sim.cpu().numpy())\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels[:len(similarities)])\n",
    "    \n",
    "    pos_sim = similarities[labels == 1]\n",
    "    neg_sim = similarities[labels == 0]\n",
    "    \n",
    "    return {\n",
    "        'avg_positive_similarity': np.mean(pos_sim),\n",
    "        'avg_negative_similarity': np.mean(neg_sim),\n",
    "        'num_positive_pairs': len(pos_sim),\n",
    "        'num_negative_pairs': len(neg_sim)\n",
    "    }\n",
    "\n",
    "\n",
    "# CELL 8: Embedding Comparison Function\n",
    "def compare_embeddings(logir_model, ffnn_model, vocab, idx2word, output_dir):\n",
    "    \"\"\"Compare embeddings from both models\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.info(\"\\nComparing embeddings between models...\")\n",
    "    \n",
    "    logir_embeddings = logir_model.embeddings.weight.data.numpy()\n",
    "    ffnn_embeddings = ffnn_model.embeddings.weight.data.numpy()\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        sim = cosine_similarity(\n",
    "            logir_embeddings[i].reshape(1, -1),\n",
    "            ffnn_embeddings[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    sorted_idx = np.argsort(similarities)\n",
    "    most_similar = [(idx2word[i], similarities[i]) for i in sorted_idx[-5:]]\n",
    "    least_similar = [(idx2word[i], similarities[i]) for i in sorted_idx[:5]]\n",
    "    \n",
    "    return {\n",
    "        'average_similarity': np.mean(similarities),\n",
    "        'std_similarity': np.std(similarities),\n",
    "        'most_similar': most_similar,\n",
    "        'least_similar': least_similar\n",
    "    }\n",
    "\n",
    "# CELL 9: Report Generation Function\n",
    "def save_comprehensive_report(logir_eval, ffnn_eval, embedding_comparison, output_dir):\n",
    "    \"\"\"Save comprehensive evaluation report\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    report_path = os.path.join(output_dir, 'evaluation_report.txt')\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(\"Word2Vec Models Evaluation Report\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Model evaluations\n",
    "        f.write(\"1. Model Evaluations\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        # LogiR results\n",
    "        f.write(\"Logistic Regression Model:\\n\")\n",
    "        f.write(f\"Average Positive Similarity: {logir_eval['avg_positive_similarity']:.4f}\\n\")\n",
    "        f.write(f\"Average Negative Similarity: {logir_eval['avg_negative_similarity']:.4f}\\n\")\n",
    "        f.write(f\"Number of Positive Pairs: {logir_eval['num_positive_pairs']}\\n\")\n",
    "        f.write(f\"Number of Negative Pairs: {logir_eval['num_negative_pairs']}\\n\\n\")\n",
    "        \n",
    "        # FFNN results\n",
    "        f.write(\"FFNN Model:\\n\")\n",
    "        f.write(f\"Average Positive Similarity: {ffnn_eval['avg_positive_similarity']:.4f}\\n\")\n",
    "        f.write(f\"Average Negative Similarity: {ffnn_eval['avg_negative_similarity']:.4f}\\n\")\n",
    "        f.write(f\"Number of Positive Pairs: {ffnn_eval['num_positive_pairs']}\\n\")\n",
    "        f.write(f\"Number of Negative Pairs: {ffnn_eval['num_negative_pairs']}\\n\\n\")\n",
    "        \n",
    "        # Embedding comparison\n",
    "        f.write(\"2. Embedding Comparison\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        f.write(f\"Average Similarity between Models: {embedding_comparison['average_similarity']:.4f}\\n\")\n",
    "        f.write(f\"Standard Deviation: {embedding_comparison['std_similarity']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Most Similar Words between Models:\\n\")\n",
    "        for word, sim in embedding_comparison['most_similar']:\n",
    "            f.write(f\"{word}: {sim:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Least Similar Words between Models:\\n\")\n",
    "        for word, sim in embedding_comparison['least_similar']:\n",
    "            f.write(f\"{word}: {sim:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Analysis summary\n",
    "        f.write(\"3. Analysis Summary\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        logir_diff = logir_eval['avg_positive_similarity'] - logir_eval['avg_negative_similarity']\n",
    "        ffnn_diff = ffnn_eval['avg_positive_similarity'] - ffnn_eval['avg_negative_similarity']\n",
    "        \n",
    "        f.write(\"Model Performance Comparison:\\n\")\n",
    "        f.write(f\"LogiR Positive-Negative Difference: {logir_diff:.4f}\\n\")\n",
    "        f.write(f\"FFNN Positive-Negative Difference: {ffnn_diff:.4f}\\n\\n\")\n",
    "        \n",
    "        better_model = \"Logistic Regression\" if logir_diff > ffnn_diff else \"FFNN\"\n",
    "        f.write(f\"The {better_model} model shows better discrimination between positive and negative pairs.\\n\\n\")\n",
    "        \n",
    "        # Embedding similarity analysis\n",
    "        f.write(\"Embedding Similarity Analysis:\\n\")\n",
    "        if embedding_comparison['average_similarity'] > 0.7:\n",
    "            f.write(\"The models learned very similar embeddings, suggesting they captured similar semantic relationships.\\n\")\n",
    "        elif embedding_comparison['average_similarity'] > 0.4:\n",
    "            f.write(\"The models learned moderately similar embeddings, with some differences in their semantic representations.\\n\")\n",
    "        else:\n",
    "            f.write(\"The models learned quite different embeddings, suggesting they captured different aspects of the semantic relationships.\\n\")\n",
    "    \n",
    "    logger.info(f\"Comprehensive evaluation report saved to: {report_path}\")\n",
    "    \n",
    "\n",
    "# Detailed embedding comparison functions\n",
    "def analyze_embeddings(model_name, embeddings, idx2word, vocab_size=1000):\n",
    "    \"\"\"Analyze embeddings of a single model\"\"\"\n",
    "    # Get most frequent words (first N words in vocabulary)\n",
    "    frequent_words = [idx2word[i] for i in range(min(vocab_size, len(idx2word)))]\n",
    "    frequent_embeddings = embeddings[:vocab_size]\n",
    "    \n",
    "    # Calculate cosine similarity matrix for frequent words\n",
    "    similarity_matrix = cosine_similarity(frequent_embeddings)\n",
    "    \n",
    "    # Find most similar word pairs\n",
    "    most_similar_pairs = []\n",
    "    for i in range(len(frequent_words)):\n",
    "        for j in range(i + 1, len(frequent_words)):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            most_similar_pairs.append((frequent_words[i], frequent_words[j], similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    most_similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'embedding_norm': np.linalg.norm(embeddings),\n",
    "        'embedding_mean': np.mean(embeddings),\n",
    "        'embedding_std': np.std(embeddings),\n",
    "        'most_similar_pairs': most_similar_pairs[:10]  # Top 10 most similar pairs\n",
    "    }\n",
    "\n",
    "def compare_model_embeddings(logir_model, ffnn_model, vocab, idx2word, output_dir):\n",
    "    \"\"\"Compare embeddings from both models and generate detailed report\"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.info(\"\\nComparing embeddings between models...\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    logir_embeddings = logir_model.embeddings.weight.data.numpy()\n",
    "    ffnn_embeddings = ffnn_model.embeddings.weight.data.numpy()\n",
    "    \n",
    "    # Analyze each model's embeddings\n",
    "    logir_analysis = analyze_embeddings(\"Logistic Regression\", logir_embeddings, idx2word)\n",
    "    ffnn_analysis = analyze_embeddings(\"FFNN\", ffnn_embeddings, idx2word)\n",
    "    \n",
    "    # Calculate overall similarity between models\n",
    "    # Sample a subset of words for efficiency\n",
    "    sample_size = min(1000, len(vocab))\n",
    "    sample_indices = np.random.choice(len(vocab), sample_size, replace=False)\n",
    "    \n",
    "    logir_sample = logir_embeddings[sample_indices]\n",
    "    ffnn_sample = ffnn_embeddings[sample_indices]\n",
    "    \n",
    "    # Calculate cosine similarity between corresponding embeddings\n",
    "    similarities = []\n",
    "    for i in range(sample_size):\n",
    "        sim = cosine_similarity(\n",
    "            logir_sample[i].reshape(1, -1),\n",
    "            ffnn_sample[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    # Generate comparison report\n",
    "    report_path = os.path.join(output_dir, 'embedding_comparison_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"Word2Vec Models Embedding Comparison Report\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Model Statistics\n",
    "        f.write(\"1. Model Statistics\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        for analysis in [logir_analysis, ffnn_analysis]:\n",
    "            f.write(f\"{analysis['model_name']} Model:\\n\")\n",
    "            f.write(f\"  Embedding Norm: {analysis['embedding_norm']:.4f}\\n\")\n",
    "            f.write(f\"  Mean: {analysis['embedding_mean']:.4f}\\n\")\n",
    "            f.write(f\"  Standard Deviation: {analysis['embedding_std']:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"  Top 10 Most Similar Word Pairs:\\n\")\n",
    "            for word1, word2, sim in analysis['most_similar_pairs']:\n",
    "                f.write(f\"    {word1:20} - {word2:20}: {sim:.4f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Model Comparison\n",
    "        f.write(\"2. Model Comparison\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        avg_similarity = np.mean(similarities)\n",
    "        std_similarity = np.std(similarities)\n",
    "        \n",
    "        f.write(f\"Average Similarity between Models: {avg_similarity:.4f}\\n\")\n",
    "        f.write(f\"Similarity Standard Deviation: {std_similarity:.4f}\\n\\n\")\n",
    "        \n",
    "        # Analysis Summary\n",
    "        f.write(\"3. Analysis and Insights\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\\n\")\n",
    "        \n",
    "        # Compare embedding distributions\n",
    "        f.write(\"Embedding Distribution Analysis:\\n\")\n",
    "        if abs(logir_analysis['embedding_mean'] - ffnn_analysis['embedding_mean']) < 0.1:\n",
    "            f.write(\"- Both models learned similar average embedding values\\n\")\n",
    "        else:\n",
    "            f.write(\"- Models learned different average embedding values\\n\")\n",
    "            \n",
    "        if abs(logir_analysis['embedding_std'] - ffnn_analysis['embedding_std']) < 0.1:\n",
    "            f.write(\"- Both models show similar embedding spread\\n\")\n",
    "        else:\n",
    "            f.write(\"- Models show different levels of embedding spread\\n\")\n",
    "        \n",
    "        # Analyze similarity patterns\n",
    "        f.write(\"\\nSimilarity Analysis:\\n\")\n",
    "        if avg_similarity > 0.7:\n",
    "            f.write(\"- Models learned very similar word representations\\n\")\n",
    "        elif avg_similarity > 0.4:\n",
    "            f.write(\"- Models learned moderately similar word representations\\n\")\n",
    "        else:\n",
    "            f.write(\"- Models learned quite different word representations\\n\")\n",
    "            \n",
    "        # Compare word relationships\n",
    "        common_pairs = set([(w1, w2) for w1, w2, _ in logir_analysis['most_similar_pairs'][:5]]).intersection(\n",
    "            set([(w1, w2) for w1, w2, _ in ffnn_analysis['most_similar_pairs'][:5]])\n",
    "        )\n",
    "        \n",
    "        f.write(f\"\\nCommon Word Relationships:\\n\")\n",
    "        f.write(f\"- Found {len(common_pairs)} common word pairs in top 5 most similar pairs\\n\")\n",
    "        \n",
    "        # Conclusion\n",
    "        f.write(\"\\nConclusion:\\n\")\n",
    "        if avg_similarity > 0.6:\n",
    "            f.write(\"The two models have learned largely similar word embeddings, suggesting they've captured similar semantic relationships despite their different architectures.\")\n",
    "        elif avg_similarity > 0.3:\n",
    "            f.write(\"The models have learned moderately different embeddings, indicating that the architectural differences led to somewhat different semantic representations.\")\n",
    "        else:\n",
    "            f.write(\"The models have learned substantially different embeddings, suggesting that the architectural differences significantly impact how semantic relationships are captured.\")\n",
    "    \n",
    "    logger.info(f\"Embedding comparison report saved to: {report_path}\")\n",
    "    return {\n",
    "        'logir_analysis': logir_analysis,\n",
    "        'ffnn_analysis': ffnn_analysis,\n",
    "        'average_similarity': avg_similarity,\n",
    "        'similarity_std': std_similarity\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_texts, vocab, idx2word, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model using cosine similarity on test data\n",
    "    Returns dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('word2vec')\n",
    "    logger.info(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.embeddings.weight.data\n",
    "    \n",
    "    # Generate test pairs\n",
    "    test_pairs = []\n",
    "    labels = []\n",
    "    for text in test_texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        pairs, pair_labels = generate_pairs(tokens, CONTEXT_WINDOW, vocab)\n",
    "        test_pairs.extend(pairs)\n",
    "        labels.extend(pair_labels)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    test_pairs = torch.LongTensor(test_pairs)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    batch_size = 1000  # Process in batches to avoid memory issues\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_pairs), batch_size):\n",
    "            batch_pairs = test_pairs[i:i + batch_size]\n",
    "            target_emb = embeddings[batch_pairs[:, 0]]\n",
    "            context_emb = embeddings[batch_pairs[:, 1]]\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            sim = torch_cosine_similarity(target_emb, context_emb, dim=1)\n",
    "            similarities.extend(sim.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels[:len(similarities)])\n",
    "    \n",
    "    pos_sim = similarities[labels == 1]\n",
    "    neg_sim = similarities[labels == 0]\n",
    "    \n",
    "    # Calculate average similarity for positive and negative pairs\n",
    "    avg_pos_sim = np.mean(pos_sim)\n",
    "    avg_neg_sim = np.mean(neg_sim)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_results = {\n",
    "        'avg_positive_similarity': avg_pos_sim,\n",
    "        'avg_negative_similarity': avg_neg_sim,\n",
    "        'num_positive_pairs': len(pos_sim),\n",
    "        'num_negative_pairs': len(neg_sim),\n",
    "        'positive_std': np.std(pos_sim),\n",
    "        'negative_std': np.std(neg_sim)\n",
    "    }\n",
    "    \n",
    "    # Save detailed results to file\n",
    "    results_file = os.path.join(output_dir, f'{model_name.lower()}_evaluation.txt')\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(f\"{model_name} Model Evaluation Results\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Number of test pairs: {len(similarities)}\\n\")\n",
    "        f.write(f\"Positive pairs: {len(pos_sim)}\\n\")\n",
    "        f.write(f\"Negative pairs: {len(neg_sim)}\\n\\n\")\n",
    "        f.write(f\"Average positive similarity: {avg_pos_sim:.4f} (±{np.std(pos_sim):.4f})\\n\")\n",
    "        f.write(f\"Average negative similarity: {avg_neg_sim:.4f} (±{np.std(neg_sim):.4f})\\n\")\n",
    "        f.write(f\"Discrimination (pos - neg): {avg_pos_sim - avg_neg_sim:.4f}\\n\")\n",
    "    \n",
    "    logger.info(f\"Evaluation results saved to: {results_file}\")\n",
    "    return eval_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        # CELL 10: Main Execution - Setup and Training\n",
    "    # Create output directory\n",
    "    output_dir = os.path.join('results')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    logger = setup_logging(output_dir)\n",
    "    logger.info(\"Starting Word2Vec training process\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    logger.info(\"Loading training data...\")\n",
    "    train_texts = load_data('train')\n",
    "\n",
    "    # Build vocabulary\n",
    "    logger.info(\"Building vocabulary...\")\n",
    "    vocab, idx2word, word_counts = build_vocab(train_texts)\n",
    "    logger.info(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Generate training pairs\n",
    "    logger.info(\"Generating training pairs...\")\n",
    "    all_pairs = []\n",
    "    all_labels = []\n",
    "    for text in train_texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        pairs, labels = generate_pairs(tokens, CONTEXT_WINDOW, vocab)\n",
    "        all_pairs.extend(pairs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    logger.info(f\"Generated {len(all_pairs)} training pairs\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = Word2VecDataset(all_pairs, all_labels)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train LogiR model\n",
    "    logger.info(\"\\nTraining Logistic Regression model...\")\n",
    "    logir_model = SimpleWord2Vec_LogiR(len(vocab), EMBEDDING_DIM)\n",
    "    trained_logir, logir_losses = train_model(logir_model, train_loader, lr=0.1)\n",
    "\n",
    "    # Train FFNN model\n",
    "    logger.info(\"\\nTraining FFNN model...\")\n",
    "    ffnn_model = SimpleWord2Vec_FFNN(len(vocab), EMBEDDING_DIM, node_size=64)\n",
    "    trained_ffnn, ffnn_losses = train_model(ffnn_model, train_loader, lr=0.1)\n",
    "\n",
    "    # CELL 11: Main Execution - Evaluation and Visualization\n",
    "    # Save models\n",
    "    torch.save(trained_logir.state_dict(), os.path.join(output_dir, 'logir_model.pt'))\n",
    "    torch.save(trained_ffnn.state_dict(), os.path.join(output_dir, 'ffnn_model.pt'))\n",
    "\n",
    "    # Start evaluation\n",
    "    logger.info(\"\\nStarting model evaluation...\")\n",
    "\n",
    "    # Load test data\n",
    "    test_texts = load_data('test')\n",
    "\n",
    "    # Evaluate both models\n",
    "    logir_eval = evaluate_model(trained_logir, test_texts, vocab, idx2word, output_dir, \"LogiR\")\n",
    "    ffnn_eval = evaluate_model(trained_ffnn, test_texts, vocab, idx2word, output_dir, \"FFNN\")\n",
    "\n",
    "    # Compare embeddings\n",
    "    embedding_comparison = compare_embeddings(trained_logir, trained_ffnn, vocab, idx2word, output_dir)\n",
    "    \n",
    "    # Save comprehensive report\n",
    "    save_comprehensive_report(logir_eval, ffnn_eval, embedding_comparison, output_dir)\n",
    "\n",
    "    # Save losses to text file\n",
    "    with open(os.path.join(output_dir, 'training_losses.txt'), 'w') as f:\n",
    "        f.write(\"LogiR Losses:\\n\")\n",
    "        f.write(\",\".join(map(str, logir_losses)))\n",
    "        f.write(\"\\n\\nFFNN Losses:\\n\")\n",
    "        f.write(\",\".join(map(str, ffnn_losses)))\n",
    "\n",
    "    # Plot and save loss charts\n",
    "    logger.info(\"\\nGenerating loss charts...\")\n",
    "    plot_losses(logir_losses, ffnn_losses, output_dir)\n",
    "\n",
    "    # Visualization\n",
    "    logger.info(\"\\nGenerating visualizations...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        logger.info(\"Attempting LogiR visualization...\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        success = visualize_embeddings(trained_logir, idx2word, num_words=30)\n",
    "        if success:\n",
    "            plt.title(\"Logistic Regression Model Embeddings\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, 'embeddings_visualization_Logit.png')\n",
    "        plt.savefig(save_path)\n",
    "        logger.info(f\"Visualization saved to: {save_path}\")\n",
    "\n",
    "        logger.info(\"\\nAttempting FFNN visualization...\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        success = visualize_embeddings(trained_ffnn, idx2word, num_words=30)\n",
    "        if success:\n",
    "            plt.title(\"FFNN Model Embeddings\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(output_dir, 'embeddings_visualization_FFNN.png')\n",
    "        plt.savefig(save_path)\n",
    "        logger.info(f\"Visualization saved to: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during visualization process: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "    logger.info(\"Training and evaluation process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6e915-6236-4527-9529-50c2ef72ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FMD",
   "language": "python",
   "name": "fmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
